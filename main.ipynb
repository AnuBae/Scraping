{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import mysql.connector\n",
    "import ssl\n",
    "\n",
    "# konksi nanti coba ditaruh diluar\n",
    "# koneksi\n",
    "db = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='',\n",
    "    database='UnNgGrape'\n",
    "    )\n",
    "cursor = db.cursor()\n",
    "\n",
    "class Scrape:\n",
    "\n",
    "    def scrapeMain(keyword):\n",
    "        # inisialisasi array untuk menampung hasil data\n",
    "        asal_situs = []\n",
    "        title_lowongan = []\n",
    "        nama_perusahaan = []\n",
    "        lokasi_perusahaan = []\n",
    "        keterangan_lowongan = []\n",
    "        skill_lowongan = []\n",
    "        benefit_lowongan = []\n",
    "        deskripsi_lowongan = []\n",
    "        stem_detail = []\n",
    "        link_lowongan = []\n",
    "\n",
    "        # pake try except (continue) kalau udah looping keyword\n",
    "        # loop scrape\n",
    "        cursor.execute(\"SELECT * FROM scrape\")\n",
    "        raw_scrape = cursor.fetchall()\n",
    "        for rowS in raw_scrape:\n",
    "            asal = rowS[0]\n",
    "            # scrape data\n",
    "            main_link1 = rowS[1]\n",
    "            main_link2 = rowS[2]\n",
    "            main_link = main_link1 + keyword + main_link2\n",
    "            tag_main = rowS[3]\n",
    "            tag_lowongan = rowS[4]\n",
    "            tag_lowongan_part = rowS[5]\n",
    "            tag_perusahaan = rowS[6]\n",
    "            tag_perusahaan_part = rowS[7]\n",
    "            tag_lokasi = rowS[8]\n",
    "            tag_lokasi_part = rowS[9]\n",
    "            tag_mainDetail = rowS[10]\n",
    "            tag_keterangan = rowS[11]\n",
    "            tag_skill = rowS[12]\n",
    "            tag_benefit = rowS[13]\n",
    "            tag_deskripsi = rowS[14]\n",
    "            breakDeskripsi = rowS[15]\n",
    "            raw_link = rowS[16]\n",
    "\n",
    "            # tab sekali jika sudah masuk looping scrape\n",
    "            r = Request(main_link, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'})\n",
    "            response = urlopen(r).read()\n",
    "            soup = BeautifulSoup(response, \"lxml\")\n",
    "            print(type(soup))\n",
    "\n",
    "            jobList = soup.find_all(\"div\", tag_main)\n",
    "            for p in jobList:\n",
    "                # link scrape dulu untuk cek data sudah pernah di scrape atau belum\n",
    "                link = raw_link+p.find('a').get('href')\n",
    "                lowongan = p.find(tag_lowongan_part, tag_lowongan).get_text()\n",
    "                # menggunakan try except karena ada beberapa perusahaan yang dirahasiakan\n",
    "                try:\n",
    "                    perusahaan = p.find(tag_perusahaan_part, tag_perusahaan).get_text()\n",
    "                except:\n",
    "                    perusahaan = \"Perusahaan Dirahasiakan\"\n",
    "\n",
    "                try:\n",
    "                    lokasi = p.find(tag_lokasi_part, tag_lokasi).get_text().replace(\"head office - \", \"\")\n",
    "                except Exception as e:\n",
    "                    lokasi = str(e)\n",
    "\n",
    "                # try print return scrapeDetail\n",
    "                keterangan, skill, benefit, deskripsi = Scrape.scrapeDetail(link, tag_mainDetail, tag_keterangan, tag_skill, tag_benefit, tag_deskripsi, breakDeskripsi)\n",
    "\n",
    "                # stemming for detail\n",
    "                raw_stem = lowongan + \" \" + perusahaan + \" \" + lokasi + \" \" + str(keterangan) + \" \" + str(skill) + \" \" + str(benefit) + \" \" + str(deskripsi)\n",
    "                stem = Scrape.stemming(raw_stem)\n",
    "\n",
    "                # push to array penampung hasil data\n",
    "                asal_situs.append(asal)\n",
    "                title_lowongan.append(lowongan)\n",
    "                nama_perusahaan.append(perusahaan)\n",
    "                lokasi_perusahaan.append(lokasi)\n",
    "                keterangan_lowongan.append(keterangan)\n",
    "                skill_lowongan.append(skill)\n",
    "                benefit_lowongan.append(benefit)\n",
    "                deskripsi_lowongan.append(deskripsi)\n",
    "                stem_detail.append(stem)\n",
    "                link_lowongan.append(link)\n",
    "\n",
    "        # tampil pakai panda\n",
    "        jobList_dict ={'asal':asal_situs, 'lowongan':title_lowongan, 'perusahaan':nama_perusahaan, 'lokasi':lokasi_perusahaan, 'keterangan':keterangan_lowongan, 'skill':skill_lowongan, 'benefit':benefit_lowongan, 'deskripsi':deskripsi_lowongan, 'stem':stem_detail, 'link':link_lowongan}\n",
    "        df = pd.DataFrame(jobList_dict,columns = ['asal', 'lowongan', 'perusahaan', 'lokasi', 'keterangan', 'skill', 'benefit', 'deskripsi', 'stem', 'link'])\n",
    "        return df.sort_values('asal',ascending=True)\n",
    "\n",
    "    def scrapeDetail(linkDetail, tag_mainDetail, tag_keterangan, tag_skill, tag_benefit, tag_deskripsi, breakDeskripsi):\n",
    "        r = Request(linkDetail, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'})\n",
    "        response = urlopen(r).read()\n",
    "        soup = BeautifulSoup(response, \"lxml\")\n",
    "\n",
    "        raw_detail = soup.find_all(\"div\", tag_mainDetail)\n",
    "        for p in raw_detail:\n",
    "            # menggunakan try except karena ada beberapa estimasi keterangan yang tidak terlampir\n",
    "            try:\n",
    "                raw_keterangan = p.find(\"div\", tag_keterangan ).get_text(separator=\". \").replace(\"Fungsi Kerja. \", \"\")\n",
    "                nltk_tokens = nltk.sent_tokenize(raw_keterangan)\n",
    "                keterangan = \"\"\n",
    "\n",
    "                for x in nltk_tokens:\n",
    "                    if x == \"Lamar.\":\n",
    "                        break\n",
    "                    keterangan = keterangan + \" \" + x\n",
    "            except:\n",
    "                keterangan = \"-\"\n",
    "            \n",
    "            try:\n",
    "                skill = p.find(\"div\", tag_skill).get_text(separator=\" \")\n",
    "            except:\n",
    "                skill = '-'\n",
    "            try:\n",
    "                benefit = p.find(\"div\", tag_benefit).get_text(separator=\" \").replace(\"Tunjangan dan keuntungan\", \"keuntungan:\")\n",
    "            except:\n",
    "                benefit = '-'\n",
    "\n",
    "            # menggunakan try except karena ada beberapa deskripsi yang NoneType\n",
    "            try:\n",
    "                # replace untuk glints\n",
    "                raw_deskripsi = p.find(\"div\", tag_deskripsi ).get_text(separator=\". \").replace(\"Informasi Penting. Pastikan perusahaan yang kamu lamar resmi dengan memeriksa website dan lowongan kerja mereka.. Read Less.\", \"\")\n",
    "                nltk_tokens = nltk.sent_tokenize(raw_deskripsi)\n",
    "                deskripsi = \"\"\n",
    "\n",
    "                for x in nltk_tokens:\n",
    "                    if x == breakDeskripsi:\n",
    "                        break\n",
    "                    deskripsi = deskripsi + \" \" + x\n",
    "            except:\n",
    "                deskripsi = \"-\"\n",
    "            \n",
    "            # give to scrapeMain\n",
    "            return keterangan, skill, benefit, deskripsi\n",
    "\n",
    "    def stemming(raw_stem):\n",
    "        # define punctuation\n",
    "        punctuations = '''!()-[]{};:=+`'\",<>./|\\?@#$%^&*_~'''\n",
    "\n",
    "        # remove punctuation from the string\n",
    "        no_punct = \"\"\n",
    "        for char in raw_stem:\n",
    "            if char not in punctuations:\n",
    "                no_punct = no_punct + char\n",
    "            else:\n",
    "                no_punct = no_punct + \" \"\n",
    "\n",
    "        # display the unpunctuated string\n",
    "        raw_stem = no_punct.lower()\n",
    "\n",
    "        # create stemmer\n",
    "        # nltk\n",
    "        ps = PorterStemmer()\n",
    "        # sastrawi\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "\n",
    "        # stemming process\n",
    "        # nltk\n",
    "        words = word_tokenize(raw_stem)\n",
    "        nltk_stemmer = \"\"\n",
    "        for w in words:\n",
    "            nltk_stemmer = nltk_stemmer + \" \" + ps.stem(w)\n",
    "        # sastrawi sekaligus return\n",
    "        return stemmer.stem(nltk_stemmer)\n",
    "\n",
    "# print(Scrape.getKeyword())\n",
    "# print(\"\\n\")\n",
    "# Scrape.scrapeMain() \n",
    "Scrape.scrapeMain(\"palembang\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20583e63f217a0dfcf174d22b20139d81f6c3a56bcabde6374eeeb6eff50f61d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
